{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 1.10.2 is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn import metrics\n",
    "from TFBP import datasets, dataset_loader_MT, test_dataset_loader_MT\n",
    "from TFBP import MTL_Model,\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    print('Torch',torch.__version__, 'is available')\n",
    "else:\n",
    "    print('Torch is not available. Process is terminated')\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Binding Prediction for ARID3A and ZBTB7A\n",
      "Searching for all hyperparameter settings...\n"
     ]
    }
   ],
   "source": [
    "# tfs = args.TF\n",
    "tfs = ['ARID3A', 'ZBTB7A']\n",
    "CodeTesting = True\n",
    "\n",
    "print('TF Binding Prediction for', tfs[0], 'and', tfs[1])\n",
    "print('Searching for all hyperparameter settings...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases : 1\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 150\n",
    "reverse_mode = False\n",
    "num_motif_detector = 16\n",
    "motif_len =24\n",
    "batch_size = 64\n",
    "beta1 = 2*10**-6\n",
    "beta2 = 2*10**-6 # hyperparameter tuning 해야함!!\n",
    "beta3 = 5*10**-6\n",
    "beta4 = 2*10**-6\n",
    "if CodeTesting:\n",
    "    pool_type = ['max']\n",
    "    dropout_rate_type = [0.2]\n",
    "    lr_adam_type = [0.01]\n",
    "    scheduler_type = [True] # use Cosine Annealing or not\n",
    "    opt_type = ['Adam'] # optimizer\n",
    "\n",
    "# total_cases = len(pool_type)*len(hidden_layer_type)*len(dropout_rate_type)*len(lr_type_sgd)*len(scheduler_type)*len(opt_type)\n",
    "total_cases = len(pool_type)*len(dropout_rate_type)*len(lr_adam_type)*len(scheduler_type)*len(opt_type)\n",
    "\n",
    "print('Total cases :', total_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARID3A idx : 0\n",
      "ZBTB7A idx : 9\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# dataset\n",
    "path = './data/encode/'\n",
    "all_dataset_names = datasets(path)\n",
    "TF_to_idx = {'ARID3A' : 0, 'CTCFL' : 1, 'ELK1' : 2, 'FOXA1' : 3, 'GABPA' : 4, 'MYC' : 5, 'REST' : 6, 'SP1' : 7, 'USF1' : 8, 'ZBTB7A' : 9}\n",
    "TF1_idx = TF_to_idx[tfs[0]]\n",
    "TF2_idx = TF_to_idx[tfs[1]]\n",
    "if(CodeTesting):\n",
    "    print(f'{tfs[0]} idx : {TF1_idx}')\n",
    "    print(f'{tfs[1]} idx : {TF2_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1_dataset_name = all_dataset_names[TF1_idx]\n",
    "tf1_train_dataset_path = tf1_dataset_name[0]\n",
    "tf1_test_dataset_path = tf1_dataset_name[1]\n",
    "tf1_name = tf1_train_dataset_path.split(path)[1].split(\"_AC\")[0]\n",
    "\n",
    "tf2_dataset_name = all_dataset_names[TF2_idx]\n",
    "tf2_train_dataset_path = tf2_dataset_name[0]\n",
    "tf2_test_dataset_path = tf2_dataset_name[1]\n",
    "tf2_name = tf2_train_dataset_path.split(path)[1].split(\"_AC\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader, valid_data_loader, all_data_loader = dataset_loader_MT(tf1_train_dataset_path, tf2_train_dataset_path, batch_size, reverse_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training\n"
     ]
    }
   ],
   "source": [
    "print('Model Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify hyperparameters\n",
    "case_num = 1\n",
    "(share, remainder) = divmod(case_num, len(opt_type))\n",
    "opt = opt_type[remainder]\n",
    "(share, remainder) = divmod(share, len(scheduler_type))\n",
    "scheduler = scheduler_type[remainder]\n",
    "(share, remainder) = divmod(share, len(lr_adam_type))\n",
    "lr = lr_adam_type[remainder]\n",
    "(share, remainder) = divmod(share, len(dropout_rate_type))\n",
    "dropout_rate = dropout_rate_type[remainder]\n",
    "(share, remainder) = divmod(share, len(pool_type))\n",
    "pool = pool_type[remainder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MTL_Model(num_motif_detector,motif_len,pool,'training',lr, dropout_rate, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "if opt == 'SGD':\n",
    "    print('not possible')\n",
    "else:\n",
    "    optimizer = torch.optim.SGD([\n",
    "        model.net.wConv1, model.net.wRect1, model.net.wConv2, model.net.wRect2,\n",
    "        model.net1.wNeu,model.net1.wNeuBias,model.net1.wHidden,model.net1.wHiddenBias,\n",
    "        model.net2.wNeu,model.net2.wNeuBias,model.net2.wHidden,model.net2.wHiddenBias\n",
    "    ], lr = lr)\n",
    "\n",
    "# scheduler\n",
    "if scheduler == True:\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
    "else:\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=1) # constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_data_loader\n",
    "valid_loader = valid_data_loader\n",
    "\n",
    "loss_best = 1000\n",
    "\n",
    "with open(\"./test/\"+'2020_4_12'+'.txt', \"a\") as file:\n",
    "    file.write('(train auc 1, train auc 2, train loss, valid auc 1, valid auc 2, valid loss)) : ')\n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_batch(data, target, task, device):\n",
    "    task1_data, task1_target = [], []\n",
    "    task2_data, task2_target = [], []\n",
    "\n",
    "    for i in range(len(task)):\n",
    "        if task[i] == 0:\n",
    "            task1_data.append(data[i].numpy())\n",
    "            task1_target.append(target[i].numpy())\n",
    "        elif task[i] == 1:\n",
    "            task2_data.append(data[i].numpy())\n",
    "            task2_target.append(target[i].numpy())\n",
    "        else:\n",
    "            print('?')\n",
    "            quit()\n",
    "\n",
    "    task1_data_array = np.array(task1_data)\n",
    "    task2_data_array = np.array(task2_data)\n",
    "    task1_data_tensor = torch.tensor(task1_data_array)\n",
    "    task2_data_tensor = torch.tensor(task2_data_array)\n",
    "\n",
    "    task1_target_array = np.array(task1_target)\n",
    "    task2_target_array = np.array(task2_target)\n",
    "    task1_target_tensor = torch.tensor(task1_target_array)\n",
    "    task2_target_tensor = torch.tensor(task2_target_array)\n",
    "\n",
    "    data1 = task1_data_tensor.to(device)\n",
    "    data2 = task2_data_tensor.to(device)\n",
    "    target1 = task1_target_tensor.to(device)\n",
    "    target2 = task2_target_tensor.to(device)\n",
    "    \n",
    "    return data1, data2, target1, target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th epoch over  150\n",
      "10 th epoch over  150\n",
      "20 th epoch over  150\n",
      "30 th epoch over  150\n",
      "40 th epoch over  150\n",
      "50 th epoch over  150\n",
      "60 th epoch over  150\n",
      "70 th epoch over  150\n",
      "80 th epoch over  150\n",
      "90 th epoch over  150\n",
      "100 th epoch over  150\n",
      "110 th epoch over  150\n",
      "120 th epoch over  150\n",
      "130 th epoch over  150\n",
      "140 th epoch over  150\n",
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    if epoch%10 == 0:\n",
    "        print(epoch, 'th epoch over ', num_epochs)\n",
    "    for idx, (data, target, task) in enumerate(train_loader):\n",
    "\n",
    "        data1, data2, target1, target2 = divide_batch(data, target, task, device)\n",
    "\n",
    "        output1 = model.forward(data1, 0)\n",
    "        output2 = model.forward(data2, 1)\n",
    "\n",
    "        # task1 loss\n",
    "        loss = F.binary_cross_entropy(torch.sigmoid(output1),target1) # BCE\n",
    "        + beta1*model.net.wConv1.norm() + beta2*model.net.wConv2.norm()  # Base\n",
    "        + beta3*model.net1.wHidden.norm() + beta4*model.net1.wNeu.norm() # classifier head\n",
    "\n",
    "        # task2 loss\n",
    "        loss += F.binary_cross_entropy(torch.sigmoid(output2),target2) \n",
    "        + beta1*model.net.wConv1.norm() + beta2*model.net.wConv2.norm() # Base\n",
    "        + beta3*model.net2.wHidden.norm() + beta4*model.net2.wNeu.norm() # classifier head\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # for train set\n",
    "        model.mode='test'\n",
    "        auc1 = []\n",
    "        auc2 = []\n",
    "        train_loss = []\n",
    "        for idx, (data, target, task) in enumerate(train_loader):\n",
    "            data1, data2, target1, target2 = divide_batch(data, target, task, device)\n",
    "\n",
    "            # Forward pass\n",
    "            output1 = model.forward(data1, 0)\n",
    "            output2 = model.forward(data2, 1)\n",
    "\n",
    "            loss = F.binary_cross_entropy(torch.sigmoid(output1),target1) + beta1*model.net.wConv1.norm() + beta2*model.net.wConv2.norm() + beta3*model.net1.wHidden.norm() + beta4*model.net1.wNeu.norm()\n",
    "            loss += F.binary_cross_entropy(torch.sigmoid(output2),target2) + beta1*model.net.wConv1.norm() + beta2*model.net.wConv2.norm() + beta3*model.net2.wHidden.norm() + beta4*model.net2.wNeu.norm()\n",
    "            train_loss.append(loss.cpu())\n",
    "\n",
    "            pred1_sig = torch.sigmoid(output1)\n",
    "            pred2_sig = torch.sigmoid(output2)\n",
    "\n",
    "            pred1 = pred1_sig.cpu().detach().numpy().reshape(output1.shape[0])\n",
    "            pred2 = pred2_sig.cpu().detach().numpy().reshape(output2.shape[0])\n",
    "\n",
    "            label1 = target1.cpu().numpy().reshape(output1.shape[0])\n",
    "            label2 = target2.cpu().numpy().reshape(output2.shape[0])\n",
    "\n",
    "            try:\n",
    "                auc1.append(metrics.roc_auc_score(label1, pred1))\n",
    "                auc2.append(metrics.roc_auc_score(label2, pred2))\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        AUC_training_1 = np.mean(auc1)\n",
    "        AUC_training_2 = np.mean(auc2)\n",
    "        Loss_train = np.mean(train_loss)\n",
    "\n",
    "        # for valid set\n",
    "        model.mode='test'\n",
    "        auc1 = []\n",
    "        auc2 = []\n",
    "        valid_loss = []\n",
    "        for idx, (data, target, task) in enumerate(valid_loader):\n",
    "            data1, data2, target1, target2 = divide_batch(data, target, task, device)\n",
    "\n",
    "            # Forward pass\n",
    "            output1 = model.forward(data1, 0)\n",
    "            output2 = model.forward(data2, 1)\n",
    "\n",
    "            loss = F.binary_cross_entropy(torch.sigmoid(output1),target1) + beta1*model.net.wConv1.norm() + beta2*model.net.wConv2.norm() + beta3*model.net1.wHidden.norm() + beta4*model.net1.wNeu.norm()\n",
    "            loss += F.binary_cross_entropy(torch.sigmoid(output2),target2) + beta1*model.net.wConv1.norm() + beta2*model.net.wConv2.norm() + beta3*model.net2.wHidden.norm() + beta4*model.net2.wNeu.norm()\n",
    "            valid_loss.append(loss.cpu())\n",
    "\n",
    "            pred1_sig=torch.sigmoid(output1)\n",
    "            pred2_sig=torch.sigmoid(output2)\n",
    "\n",
    "            pred1 = pred1_sig.cpu().detach().numpy().reshape(output1.shape[0])\n",
    "            pred2 = pred2_sig.cpu().detach().numpy().reshape(output2.shape[0])\n",
    "\n",
    "            label1 = target1.cpu().numpy().reshape(output1.shape[0])\n",
    "            label2 = target2.cpu().numpy().reshape(output2.shape[0])\n",
    "\n",
    "            try:\n",
    "                auc1.append(metrics.roc_auc_score(label1, pred1))\n",
    "                auc2.append(metrics.roc_auc_score(label2, pred2))\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        AUC_valid_1 = np.mean(auc1)\n",
    "        AUC_valid_2 = np.mean(auc2)\n",
    "        Loss_valid = np.mean(valid_loss)\n",
    "\n",
    "        with open(\"./test/\"+'2020_4_12'+'.txt', \"a\") as file:\n",
    "            file.write('(')\n",
    "            file.write(str(AUC_training_1))\n",
    "            file.write(',')\n",
    "            file.write(str(AUC_training_2))\n",
    "            file.write(',')\n",
    "            file.write(str(Loss_train))\n",
    "            file.write(',')\n",
    "            file.write(str(AUC_valid_1))\n",
    "            file.write(',')\n",
    "            file.write(str(AUC_valid_2))\n",
    "            file.write(',')\n",
    "            file.write(str(Loss_valid))\n",
    "            file.write(')')\n",
    "            file.write('\\n')\n",
    "        file.close()\n",
    "\n",
    "        if Loss_valid < loss_best:\n",
    "            loss_best = Loss_valid\n",
    "            best_model = model\n",
    "            state = {'conv1': model.net.wConv1,\n",
    "                    'rect1':model.net.wRect1,\n",
    "                    'conv2':model.net.wConv2,\n",
    "                    'rect2':model.net.wRect2,\n",
    "                    'wHidden1':model.net1.wHidden,\n",
    "                    'wHiddenBias1':model.net1.wHiddenBias,\n",
    "                    'wNeu1':model.net1.wNeu,\n",
    "                    'wNeuBias1':model.net1.wNeuBias,\n",
    "                    'wHidden2':model.net2.wHidden,\n",
    "                    'wHiddenBias2':model.net2.wHiddenBias,\n",
    "                    'wNeu2':model.net2.wNeu,\n",
    "                    'wNeuBias2':model.net2.wNeuBias}\n",
    "\n",
    "            isExist = os.path.exists('./Models/' + '2020_4_12')\n",
    "            if not isExist:\n",
    "                os.makedirs('./Models/' + '2020_4_12')\n",
    "\n",
    "            torch.save(state, './Models/' + '2020_4_12'+ '/' + str(1) + '.pth')\n",
    "\n",
    "print('Training Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Testing\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "print('Model Testing')\n",
    "\n",
    "test_loader = test_dataset_loader_MT(tf1_test_dataset_path, tf2_test_dataset_path, motif_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./Models/' + '2020_4_12'+ '/' + str(1) + '.pth')\n",
    "model = MTL_Model(num_motif_detector, motif_len, pool, 'testing', lr, dropout_rate, device)\n",
    "\n",
    "model.net.wConv1 = checkpoint['conv1']\n",
    "model.net.wRect1 = checkpoint['rect1']\n",
    "model.net.wConv2 = checkpoint['conv2']\n",
    "model.net.wRect2 = checkpoint['rect2']\n",
    "\n",
    "model.net1.wHidden = checkpoint['wHidden1']\n",
    "model.net1.wHiddenBias = checkpoint['wHiddenBias1']\n",
    "model.net1.wNeu = checkpoint['wNeu1']\n",
    "model.net1.wNeuBias = checkpoint['wNeuBias1']\n",
    "\n",
    "model.net2.wHidden = checkpoint['wHidden2']\n",
    "model.net2.wHiddenBias = checkpoint['wHiddenBias2']\n",
    "model.net2.wNeu = checkpoint['wNeu2']\n",
    "model.net2.wNeuBias = checkpoint['wNeuBias2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (data, target, task) in enumerate(test_loader):\n",
    "    data1, data2, target1, target2 = divide_batch(data, target, task, device)\n",
    "    if idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on test data =  0.5652440000000001 0.571552\n",
      "Testing Completed\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_auc_1 = []\n",
    "    test_auc_2 = []\n",
    "    \n",
    "    for idx, (data, target, task) in enumerate(test_loader):\n",
    "        data1, data2, target1, target2 = divide_batch(data, target, task, device)\n",
    "\n",
    "        # Forward pass\n",
    "        output1 = model.forward(data1, 0)\n",
    "        output2 = model.forward(data2, 1)\n",
    "\n",
    "        pred1_sig=torch.sigmoid(output1)\n",
    "        pred2_sig=torch.sigmoid(output2)\n",
    "\n",
    "        pred1 = pred1_sig.cpu().detach().numpy().reshape(output1.shape[0])\n",
    "        pred2 = pred2_sig.cpu().detach().numpy().reshape(output2.shape[0])\n",
    "\n",
    "        label1 = target1.cpu().numpy().reshape(output1.shape[0])\n",
    "        label2 = target2.cpu().numpy().reshape(output2.shape[0])\n",
    "\n",
    "        try:\n",
    "            test_auc_1.append(metrics.roc_auc_score(label1, pred1))\n",
    "            test_auc_2.append(metrics.roc_auc_score(label2, pred2))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    AUC_test_1 = np.mean(test_auc_1)\n",
    "    AUC_test_2 = np.mean(test_auc_2)\n",
    "    print('AUC on test data = ', AUC_test_1, AUC_test_2)\n",
    "\n",
    "    with open(\"./test/\"+'2020_4_12'+'.txt', \"a\") as file:\n",
    "        file.write('AUC Test 1 : ')\n",
    "        file.write(str(AUC_test_1))\n",
    "        file.write(\", \")\n",
    "        file.write('AUC Test 2 : ')\n",
    "        file.write(str(AUC_test_1))\n",
    "        file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "print('Testing Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84c63b404c7ee130c9845246a39403d500621500816a8f9744527cdc65d245ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('DeepBind')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
